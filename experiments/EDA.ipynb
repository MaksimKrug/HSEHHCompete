{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc69447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, NewsMorphTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3af8a9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/HeadHunter_train.csv\")\n",
    "test = pd.read_csv(\"../data/HeadHunter_test.csv\")\n",
    "sample_submission = pd.read_csv(\"../data/HeadHunter_sample_submit.csv\")\n",
    "\n",
    "print(f\"Train shape: {train.shape} | Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29546766",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c76d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([i[0] for i in train[[\"target\"]].value_counts(normalize=True).iloc[:5].index.values],\n",
    "        train[[\"target\"]].value_counts(normalize=True).iloc[:5].values)\n",
    "plt.ylabel(\"Percent of target\")\n",
    "plt.xlabel(\"Target Name\")\n",
    "plt.title(\"Target distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937578d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metadata distribution\n",
    "metadata_columns = [\"salary_rating\", \"team_rating\", \"managment_rating\", \"career_rating\",\n",
    "                    \"workplace_rating\", \"rest_recovery_rating\"]\n",
    "for feature in metadata_columns:\n",
    "    plt.bar([i[0] for i in train[[feature]].value_counts(normalize=True).index.values],\n",
    "            train[[feature]].value_counts(normalize=True).values)\n",
    "    plt.ylabel(f\"Percent of {feature}\")\n",
    "    plt.xlabel(f\"{feature}\")\n",
    "    plt.title(f\"{feature} distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation plot\n",
    "train[\"preprocessed_target\"] = train[\"target\"].str.split(\",\").apply(lambda x: x[0]).astype(int)\n",
    "corr = train[metadata_columns + [\"preprocessed_target\"]].corr()\n",
    "\n",
    "plt.figure(figsize=(13, 7))\n",
    "sns.heatmap(corr, vmin=-1, vmax=1, annot=True)\n",
    "plt.title(\"Metadata and Target correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata by target\n",
    "train[\"metadata_sum\"] = train[metadata_columns].sum(axis=1)\n",
    "train.groupby([\"target\"])[\"metadata_sum\"].agg([\"mean\", \"median\", \"max\", \"min\", \"count\"]).sort_values(\"mean\", ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e96db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# city\n",
    "plt.bar([i[0] for i in train[[\"city\"]].value_counts(normalize=True).iloc[:5].index.values],\n",
    "       train[[\"city\"]].value_counts(normalize=True).iloc[:5].values)\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.xticks(rotation=-45)\n",
    "plt.title(\"City distribution\")\n",
    "plt.show()\n",
    "\n",
    "# position\n",
    "plt.bar([i[0] for i in train[[\"position\"]].value_counts(normalize=True).iloc[:5].index.values],\n",
    "       train[[\"position\"]].value_counts(normalize=True).iloc[:5].values)\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.xticks(rotation=-45)\n",
    "plt.title(\"Position distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA\n",
    "print(\"NaNs sum\")\n",
    "display(train.isna().sum())\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(\"NaNs Postive\")\n",
    "display(train[train[\"positive\"].isna()==True][\"target\"].value_counts())\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(\"NaNs Negative\")\n",
    "display(train[train[\"negative\"].isna()==True][\"target\"].value_counts())\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(\"NaNs Both\")\n",
    "display(train[(train[\"positive\"].isna()==True)\n",
    "             &((train[\"negative\"].isna()==True))][\"target\"].value_counts())\n",
    "\n",
    "# Test distribution\n",
    "print(\"NaNs sum\")\n",
    "display(test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nans\n",
    "train.dropna(subset=[\"positive\", \"negative\"], inplace=True, how=\"all\")\n",
    "train.loc[train[\"negative\"].isna(), \"negative\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6758932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sent:str, lowercase:bool=True,\n",
    "                  remove_punctuation:bool=False, remove_stopwords:bool=False,\n",
    "                  lemmatize:bool=False):\n",
    "    # lowercase\n",
    "    if lowercase:\n",
    "        sent = sent.lower()\n",
    "        \n",
    "    # remove_punctuation\n",
    "    if remove_punctuation:\n",
    "        tokenizer = nltk.RegexpTokenizer(r'[а-я]+')\n",
    "        sent = \" \".join(tokenizer.tokenize(sent))\n",
    "    \n",
    "    # remove_stopwords\n",
    "    if remove_stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words(\"russian\")\n",
    "        sent = \" \".join([w for w in sent.split() if w not in stopwords])\n",
    "        \n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "        doc = Doc(sent)\n",
    "        # Segmentation\n",
    "        doc.segment(segmenter)\n",
    "\n",
    "        # Morphology\n",
    "        morph_tagger = NewsMorphTagger(emb)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "\n",
    "        # Lemmatization\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(morph_vocab)\n",
    "        sent = \" \".join([w.lemma for w in doc.tokens])\n",
    "        \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd543189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab sentences\n",
    "positive_sentences_raw = train[\"positive\"].tolist()\n",
    "negative_sentences_raw = train[\"negative\"].tolist()\n",
    "\n",
    "# init empty lists\n",
    "positive_sentences = []\n",
    "negative_sentences = []\n",
    "\n",
    "# natasha utils\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "# preprocess\n",
    "for sent in tqdm(positive_sentences_raw):\n",
    "    sent = preprocessing(sent, lowercase=True, remove_punctuation=True, remove_stopwords=True,\n",
    "                         lemmatize=True)\n",
    "    positive_sentences.append(sent)\n",
    "    del sent\n",
    "    \n",
    "for sent in tqdm(negative_sentences_raw):\n",
    "    sent = preprocessing(sent, lowercase=True, remove_punctuation=True, remove_stopwords=True,\n",
    "                         lemmatize=True)\n",
    "    negative_sentences.append(sent)\n",
    "    del sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to npy\n",
    "positive_sentences = np.array(positive_sentences)\n",
    "negative_sentences = np.array(negative_sentences)\n",
    "\n",
    "np.save(\"data/positive_sentences\", positive_sentences)\n",
    "np.save(\"data/negative_sentences\", negative_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffacc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words Frequency Positive\n",
    "wordfreq = nltk.FreqDist([w for sent in positive_sentences for w in sent.split()])\n",
    "wordfreq = {k: v for k, v in sorted(dict(wordfreq).items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(list(wordfreq.keys())[:30], list(wordfreq.values())[:30])\n",
    "plt.title(\"Words frequency Positive\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()\n",
    "\n",
    "# Words Frequency Negative\n",
    "wordfreq = nltk.FreqDist([w for sent in negative_sentences for w in sent.split()])\n",
    "wordfreq = {k: v for k, v in sorted(dict(wordfreq).items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(list(wordfreq.keys())[:30], list(wordfreq.values())[:30])\n",
    "plt.title(\"Words frequency Negative\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4bb41",
   "metadata": {},
   "source": [
    "1) We don't need rows with missed both \"positive\" and \"negative\"\\\n",
    "2) More then 80% of targets are 0 or 8 values, it's would be great to train binary classification model\\\n",
    "3) We have multilabel but i'm not sure that we can submit multilabel\\\n",
    "4) We need somehow concat \"positive\", \"negative\" and \"metadata\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae05290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac5af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f6082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c036712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b3874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0ed5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c3d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
