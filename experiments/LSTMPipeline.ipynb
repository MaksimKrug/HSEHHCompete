{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f62513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484ecee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49dfa243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "path = \"data/navec_hudlit_v1_12B_500K_300d_100q.tar\"\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64473f44",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68381dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (50876, 12) | Test shape: (50651, 11)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/HeadHunter_train.csv\")\n",
    "test = pd.read_csv(\"data/HeadHunter_test.csv\")\n",
    "sample_submission = pd.read_csv(\"data/HeadHunter_sample_submit.csv\")\n",
    "\n",
    "print(f\"Train shape: {train.shape} | Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165a6f3",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a31567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_SIZE = 102\n",
    "META_SIZE = 4\n",
    "METADATA_SIZE = 31\n",
    "VEC_SIZE = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cfbf1",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5816945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNs preprocessing\n",
    "train.fillna(value={\"city\":\"<unk>\", \"position\":\"<unk>\", \"positive\":\"<unk>\", \"negative\":\"<unk>\"}, inplace=True)\n",
    "test.fillna(value={\"city\":\"<unk>\", \"position\":\"<unk>\", \"positive\":\"<unk>\", \"negative\":\"<unk>\"}, inplace=True)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883cbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features preprocessing\n",
    "# lower city and position\n",
    "train[[\"city\", \"position\", \"positive\", \"negative\"]] = train[[\"city\", \"position\", \"positive\", \"negative\"]].apply(lambda x: x.str.lower())\n",
    "test[[\"city\", \"position\", \"positive\", \"negative\"]] = test[[\"city\", \"position\", \"positive\", \"negative\"]].apply(lambda x: x.str.lower())\n",
    "\n",
    "# standard scaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler_columns = [\"salary_rating\", \"team_rating\", \"managment_rating\",\n",
    "#                   \"career_rating\", \"workplace_rating\", \"rest_recovery_rating\"]\n",
    "# train[scaler_columns] = scaler.fit_transform(train[scaler_columns])\n",
    "# test[scaler_columns] = scaler.transform(test[scaler_columns])\n",
    "\n",
    "# One Hot\n",
    "concat_temp = pd.concat((train, test))\n",
    "metadata_columns = [\"salary_rating\", \"team_rating\", \"managment_rating\",\n",
    "                    \"career_rating\", \"workplace_rating\", \"rest_recovery_rating\"]\n",
    "concat_temp = pd.get_dummies(concat_temp, columns=metadata_columns)\n",
    "dummies_columns = [i for i in concat_temp.columns if len([j for j in metadata_columns if j in i]) != 0]\n",
    "train = concat_temp.loc[concat_temp[\"target\"].notna()]\n",
    "test = concat_temp.loc[concat_temp[\"target\"].isna()]\n",
    "\n",
    "# target to single label\n",
    "train[\"preprocessed_target\"] = train[\"target\"].str.split(\",\").apply(lambda x: x[0]).astype(int)\n",
    "\n",
    "# reset index\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9803aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.64 s, sys: 15.8 ms, total: 9.66 s\n",
      "Wall time: 9.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vocab\n",
    "tokenizer = nltk.RegexpTokenizer(r\"[а-я]+|<unk>|[a-z]+\")\n",
    "word2idx = {\"<pad>\":0, \"<unk>\":1}\n",
    "idx = 2\n",
    "\n",
    "# create vocab\n",
    "for text_column in [\"city\", \"position\", \"positive\", \"negative\"]:\n",
    "    text = train[text_column].values\n",
    "    tokens = [tokenizer.tokenize(sent) for sent in text]\n",
    "    text_column_sentences = []    \n",
    "    for idx, sent in enumerate(tokens):        \n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            word_emb = navec.get(word)\n",
    "            if word_emb is None:\n",
    "                new_sent.append(word2idx[\"<unk>\"])\n",
    "            elif word not in word2idx:\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                new_sent.append(word2idx[word])\n",
    "            else:\n",
    "                new_sent.append(word2idx[word])\n",
    "        # update text_column list\n",
    "        if new_sent == []:\n",
    "            new_sent = [word2idx[\"<unk>\"]]\n",
    "        text_column_sentences.append(new_sent)\n",
    "    # update dataframe\n",
    "    train[text_column] = text_column_sentences\n",
    "    \n",
    "# test preprocessing\n",
    "for text_column in [\"city\", \"position\", \"positive\", \"negative\"]:\n",
    "    text = test[text_column].values\n",
    "    tokens = [tokenizer.tokenize(sent) for sent in text]\n",
    "    text_column_sentences = []    \n",
    "    for idx, sent in enumerate(tokens):        \n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            if word in word2idx:\n",
    "                word_emb = word2idx[word]\n",
    "            else:\n",
    "                word_emb = word2idx[\"<unk>\"]\n",
    "            new_sent.append(word_emb)\n",
    "        # update text_column list\n",
    "        if new_sent == []:\n",
    "            new_sent = [word2idx[\"<unk>\"]]\n",
    "        text_column_sentences.append(new_sent)\n",
    "    # update dataframe\n",
    "    test[text_column] = text_column_sentences\n",
    " \n",
    "# emb weights\n",
    "vocab = list(word2idx.keys())\n",
    "emb_weights = np.zeros((len(vocab), VEC_SIZE))\n",
    "                     \n",
    "for idx, (word, word_idx) in enumerate(word2idx.items()):\n",
    "    emb_weights[idx] = navec.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44cb15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (45788, 38), Val Shape: (5088, 38)\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "train, val = train_test_split(train, test_size=0.1, stratify=train[\"preprocessed_target\"], shuffle=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Train Shape: {train.shape}, Val Shape: {val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b36e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, train_mode):\n",
    "        # utils\n",
    "        metadata_columns = [\"salary_rating\", \"team_rating\", \"managment_rating\",\n",
    "                            \"career_rating\", \"workplace_rating\", \"rest_recovery_rating\"]\n",
    "        # for one hot\n",
    "        metadata_columns = [i for i in df.columns if len([j for j in metadata_columns if j in i]) != 0]\n",
    "        self.tokenizer = nltk.RegexpTokenizer(r\"[а-я]+|<unk>|<pad>\")\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "        # init features\n",
    "        self.positive = df[\"positive\"].values\n",
    "        self.negative = df[\"negative\"].values\n",
    "        self.cities = df[\"city\"].values\n",
    "        self.position = df[\"position\"].values\n",
    "        self.metadata = df[metadata_columns].values\n",
    "        if self.train_mode:\n",
    "            self.target = df[\"preprocessed_target\"].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positive)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get sent\n",
    "        tokens_positive, tokens_negative = self.positive[idx], self.negative[idx]\n",
    "        tokens_city, tokens_position = self.cities[idx], self.position[idx]\n",
    "        metadata = self.metadata[idx]\n",
    "        \"\"\"\n",
    "        For text:\n",
    "\n",
    "        \"\"\"\n",
    "        # padding\n",
    "        tokens_positive = np.pad(tokens_positive[:SENT_SIZE],\n",
    "                                 pad_width=(max(0, SENT_SIZE - len(tokens_positive)), 0), \n",
    "                                 constant_values=(word2idx[\"<pad>\"], word2idx[\"<pad>\"]))\n",
    "        tokens_negative = np.pad(tokens_negative[:SENT_SIZE],\n",
    "                                 pad_width=(max(0, SENT_SIZE - len(tokens_negative)), 0), \n",
    "                                 constant_values=(word2idx[\"<pad>\"], word2idx[\"<pad>\"]))   \n",
    "        tokens_city = np.pad(tokens_city[:META_SIZE],\n",
    "                             pad_width=(max(0, META_SIZE - len(tokens_city)), 0), \n",
    "                             constant_values=(word2idx[\"<pad>\"], word2idx[\"<pad>\"]))\n",
    "        tokens_position = np.pad(tokens_position[:META_SIZE],\n",
    "                                   pad_width=(max(0, META_SIZE - len(tokens_position)), 0), \n",
    "                                   constant_values=(word2idx[\"<pad>\"], word2idx[\"<pad>\"]))\n",
    "        # stack tokens\n",
    "        tokens_positive = np.stack(tokens_positive)\n",
    "        tokens_negative = np.stack(tokens_negative)\n",
    "        tokens_city = np.stack(tokens_city)\n",
    "        tokens_position = np.stack(tokens_position)\n",
    "        # cnvert tokens 2 Long\n",
    "        tokens_positive = torch.LongTensor(tokens_positive)\n",
    "        tokens_negative = torch.LongTensor(tokens_negative)\n",
    "        tokens_meta = torch.cat((torch.LongTensor(tokens_city),\n",
    "                                 torch.LongTensor(tokens_position)), dim=0)\n",
    "        \n",
    "        \"\"\"\n",
    "        For target\n",
    "        \"\"\"        \n",
    "        if self.train_mode:\n",
    "            target = self.target[idx]\n",
    "            return tokens_positive, tokens_negative, tokens_meta, torch.FloatTensor(metadata), target\n",
    "        else:\n",
    "            return tokens_positive, tokens_negative, tokens_meta, torch.FloatTensor(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7d835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "dataset_train = CustomDataset(train, train_mode=True)\n",
    "dataset_val = CustomDataset(val, train_mode=True)\n",
    "dataset_test = CustomDataset(test, train_mode=False)\n",
    "dataset_fulltrain = CustomDataset(pd.concat((train, val)), train_mode=True)\n",
    "\n",
    "# create dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256, shuffle=True)\n",
    "dataloader_fulltrain = DataLoader(dataset_fulltrain, batch_size=256, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99046107",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens_positive, tokens_negative, tokens_meta, metadata, target in dataloader_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c01b1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbb2f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(emb_weights, train_embed=False):\n",
    "    \"\"\"\n",
    "    Create embeddings\n",
    "    \"\"\"\n",
    "    num_embeddings, embedding_dim = emb_weights.shape\n",
    "    emb_layer = nn.Embedding.from_pretrained(torch.from_numpy(emb_weights))\n",
    "    emb_layer.weight.requires_grad = train_embed\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74670082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(pl.LightningModule,):\n",
    "    def __init__(self, hidden_size=4, bidirectional=True):\n",
    "        super().__init__()\n",
    "        # utils\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.metric_accuracy = torchmetrics.Accuracy()\n",
    "        self.metric_f1 = torchmetrics.F1(num_classes=9, average=\"macro\")\n",
    "        \n",
    "        # logs\n",
    "        self.train_accuracy_log, self.train_f1_log, self.train_loss_log = [], [], []\n",
    "        self.val_accuracy_log, self.val_f1_log, self.val_loss_log = [], [], []\n",
    "        \n",
    "        # model\n",
    "        self.emb_layer_positive, _, self.embedding_dim = create_emb_layer(emb_weights, train_embed=True)\n",
    "        self.emb_layer_negative, _, self.embedding_dim = create_emb_layer(emb_weights, train_embed=True)\n",
    "        self.emb_layer_meta, _, self.embedding_dim = create_emb_layer(emb_weights, train_embed=True)\n",
    "        \n",
    "        self.lstm_layer_positive = nn.LSTM(input_size=VEC_SIZE, hidden_size=self.hidden_size,\n",
    "                                           bidirectional=self.bidirectional, dropout=0.2)\n",
    "        self.lstm_layer_negative = nn.LSTM(input_size=VEC_SIZE, hidden_size=self.hidden_size,\n",
    "                                           bidirectional=self.bidirectional, dropout=0.2)\n",
    "        self.lstm_layer_meta = nn.LSTM(input_size=VEC_SIZE, hidden_size=self.hidden_size,\n",
    "                                       bidirectional=self.bidirectional, dropout=0.2)\n",
    "        \n",
    "        self.linear1_positive = nn.Linear(SENT_SIZE*self.hidden_size*(self.bidirectional+1), 256)\n",
    "        self.linear1_negative = nn.Linear(SENT_SIZE*self.hidden_size*(self.bidirectional+1), 256)\n",
    "        self.linear1_meta = nn.Linear(2*META_SIZE*self.hidden_size*(self.bidirectional+1), 256)\n",
    "        self.linear1_metadata = nn.Linear(METADATA_SIZE, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4*256, 9) \n",
    "        \n",
    "        # extra utils\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batchnorm_positive = nn.BatchNorm1d(SENT_SIZE)\n",
    "        self.batchnorm_negative = nn.BatchNorm1d(SENT_SIZE)\n",
    "        self.batchnorm_meta = nn.BatchNorm1d(2*META_SIZE)\n",
    "        \n",
    "    def forward(self, tokens_positive, tokens_negative, tokens_meta, metadata):        \n",
    "        # embeddings\n",
    "        emb_positive = self.emb_layer_positive(tokens_positive)\n",
    "        emb_negative = self.emb_layer_negative(tokens_negative)\n",
    "        emb_meta = self.emb_layer_meta(tokens_meta)        \n",
    "        \n",
    "        # dropout + batchnorm\n",
    "        emb_positive = self.dropout(emb_positive)\n",
    "        emb_negative = self.dropout(emb_negative)\n",
    "        emb_meta = self.dropout(emb_meta)\n",
    "        \n",
    "        emb_positive = self.batchnorm_positive(emb_positive.float())\n",
    "        emb_negative = self.batchnorm_negative(emb_negative.float())\n",
    "        emb_meta = self.batchnorm_meta(emb_meta.float())\n",
    "           \n",
    "        # lstm\n",
    "        lstm_out_positive, (h_n, c_n) = self.lstm_layer_positive(emb_positive.float())\n",
    "        lstm_out_negative, (h_n, c_n) = self.lstm_layer_negative(emb_negative.float())\n",
    "        lstm_out_meta, (h_n, c_n) = self.lstm_layer_meta(emb_meta.float())\n",
    "                \n",
    "        # reshape\n",
    "        fc_input_positive = torch.reshape(lstm_out_positive,\n",
    "                                          (lstm_out_positive.shape[0],\n",
    "                                           SENT_SIZE*self.hidden_size*(self.bidirectional+1)))\n",
    "        fc_input_negative = torch.reshape(lstm_out_negative,\n",
    "                                          (lstm_out_negative.shape[0],\n",
    "                                           SENT_SIZE*self.hidden_size*(self.bidirectional+1)))\n",
    "        fc_input_meta = torch.reshape(lstm_out_meta, \n",
    "                                      (lstm_out_meta.shape[0],\n",
    "                                       lstm_out_meta.shape[1]*lstm_out_meta.shape[2]))\n",
    "        \n",
    "        # fc\n",
    "        x_positive = self.linear1_positive(fc_input_positive)\n",
    "        x_negative = self.linear1_negative(fc_input_negative)\n",
    "        x_meta = self.linear1_meta(fc_input_meta)\n",
    "        x_metadata = self.linear1_metadata(metadata)\n",
    "        x = torch.cat((x_positive, x_negative, x_meta, x_metadata), dim=1)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        #learning rate scheduler\n",
    "        return {\"optimizer\":optimizer,\n",
    "                \"lr_scheduler\" : {\"scheduler\" : sch}\n",
    "               }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tokens_positive, tokens_negative, tokens_meta, metadata, y = batch\n",
    "        out = self(tokens_positive, tokens_negative, tokens_meta, metadata)\n",
    "        loss = torch.nn.CrossEntropyLoss()(out, y)\n",
    "        accuracy = self.metric_accuracy(out, y)\n",
    "        f1 = self.metric_f1(out, y)\n",
    "        \n",
    "        # save logs\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", accuracy, prog_bar=True)\n",
    "        self.log(\"train_f1\", f1, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss, \"accuracy\": accuracy, \"F1\":f1}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tokens_positive, tokens_negative, tokens_meta, metadata, y = batch\n",
    "        out = self(tokens_positive, tokens_negative, tokens_meta, metadata)\n",
    "        loss = torch.nn.CrossEntropyLoss()(out, y)\n",
    "        accuracy = self.metric_accuracy(out, y)\n",
    "        f1 = self.metric_f1(out, y)\n",
    "        \n",
    "        # save logs\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_accuracy\", accuracy, prog_bar=True)\n",
    "        self.log(\"val_f1\", f1, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss, \"accuracy\": accuracy, \"F1\":f1}\n",
    "        \n",
    "    def training_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.train_loss_log.append(np.mean([i[\"loss\"].item() for i in outs]))\n",
    "        self.train_accuracy_log.append(np.mean([i[\"accuracy\"].cpu() for i in outs]))\n",
    "        self.train_f1_log.append(np.mean([i[\"F1\"].cpu() for i in outs]))\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.val_loss_log.append(np.mean([i[\"loss\"].item() for i in outs]))\n",
    "        self.val_accuracy_log.append(np.mean([i[\"accuracy\"].cpu() for i in outs]))\n",
    "        self.val_f1_log.append(np.mean([i[\"F1\"].cpu() for i in outs]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963d374",
   "metadata": {},
   "source": [
    "### Train with split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ed2c08a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                | Type        | Params\n",
      "-----------------------------------------------------\n",
      "0  | metric_accuracy     | Accuracy    | 0     \n",
      "1  | metric_f1           | F1          | 0     \n",
      "2  | emb_layer_positive  | Embedding   | 16.2 M\n",
      "3  | emb_layer_negative  | Embedding   | 16.2 M\n",
      "4  | emb_layer_meta      | Embedding   | 16.2 M\n",
      "5  | lstm_layer_positive | LSTM        | 9.8 K \n",
      "6  | lstm_layer_negative | LSTM        | 9.8 K \n",
      "7  | lstm_layer_meta     | LSTM        | 9.8 K \n",
      "8  | linear1_positive    | Linear      | 209 K \n",
      "9  | linear1_negative    | Linear      | 209 K \n",
      "10 | linear1_meta        | Linear      | 16.6 K\n",
      "11 | linear1_metadata    | Linear      | 8.2 K \n",
      "12 | relu                | ReLU        | 0     \n",
      "13 | linear2             | Linear      | 9.2 K \n",
      "14 | dropout             | Dropout     | 0     \n",
      "15 | batchnorm_positive  | BatchNorm1d | 204   \n",
      "16 | batchnorm_negative  | BatchNorm1d | 204   \n",
      "17 | batchnorm_meta      | BatchNorm1d | 16    \n",
      "-----------------------------------------------------\n",
      "49.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "49.1 M    Total params\n",
      "196.455   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a528668e1b41ae9531818f852e7ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 2s, sys: 45.1 s, total: 19min 47s\n",
      "Wall time: 19min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lstm_model = LSTMModel()\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode = \"min\", dirpath=\"data/\", filename=\"bilstm\")\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\", name=\"lstm\", version=1)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=30, logger=logger,\n",
    "                     default_root_dir=\"data/\")\n",
    "trainer.fit(lstm_model, dataloader_train, dataloader_val)\n",
    "\n",
    "# save model\n",
    "trainer.save_checkpoint(\"data/models/BILstm.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c07ba691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_checkpoint(\"data/models/BILstm.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36766399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b623e5c257ed4d9bb58d70377b41761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final model\n",
    "preds = []\n",
    "\n",
    "for tokens_positive, tokens_negative, tokens_meta, metadata in tqdm(dataloader_test):\n",
    "    pred = lstm_model(tokens_positive, tokens_negative, tokens_meta, metadata)\n",
    "    pred_target = torch.argmax(pred).item()\n",
    "    preds.append(pred_target)\n",
    "    \n",
    "sample_submission[\"target\"] = preds\n",
    "sample_submission.to_csv(\"data/submissions/submission_bilstm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "677c0f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    0.452745\n",
       "0    0.406428\n",
       "1    0.093443\n",
       "3    0.022428\n",
       "6    0.016525\n",
       "7    0.003968\n",
       "5    0.003672\n",
       "4    0.000730\n",
       "2    0.000059\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514035cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8638b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cacfd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832ac86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26518fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
