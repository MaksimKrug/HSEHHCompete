{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8121ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbca6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a82c7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8cf117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (50876, 12) | Test shape: (50651, 11)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/HeadHunter_train.csv\")\n",
    "test = pd.read_csv(\"data/HeadHunter_test.csv\")\n",
    "sample_submission = pd.read_csv(\"data/HeadHunter_sample_submit.csv\")\n",
    "\n",
    "print(f\"Train shape: {train.shape} | Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980b938",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4916123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_SIZE = 102 # q_95\n",
    "META_SIZE = 6\n",
    "METADATA_SIZE = 31\n",
    "VEC_SIZE = 300\n",
    "\n",
    "# TOKENIZER = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"SkolkovoInstitute/russian_toxicity_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955aef8",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c4e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e057d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# NaNs preprocessing\n",
    "train.fillna(value={\"city\":\"[UNK]\", \"position\":\"[UNK]\", \"positive\":\"[UNK]\", \"negative\":\"[UNK]\"}, inplace=True)\n",
    "test.fillna(value={\"city\":\"[UNK]\", \"position\":\"[UNK]\", \"positive\":\"[UNK]\", \"negative\":\"[UNK]\"}, inplace=True) \n",
    "\n",
    "# lowercase\n",
    "train[[\"positive\", \"negative\"]] = train[[\"positive\", \"negative\"]].apply(lambda x: x.str.lower())\n",
    "test[[\"positive\", \"negative\"]] = test[[\"positive\", \"negative\"]].apply(lambda x: x.str.lower())\n",
    "\n",
    "# One Hot\n",
    "concat_temp = pd.concat((train, test))\n",
    "metadata_columns = [\"salary_rating\", \"team_rating\", \"managment_rating\",\n",
    "                    \"career_rating\", \"workplace_rating\", \"rest_recovery_rating\"]\n",
    "concat_temp = pd.get_dummies(concat_temp, columns=metadata_columns)\n",
    "dummies_columns = [i for i in concat_temp.columns if len([j for j in metadata_columns if j in i]) != 0]\n",
    "train = concat_temp.loc[concat_temp[\"target\"].notna()]\n",
    "test = concat_temp.loc[concat_temp[\"target\"].isna()]\n",
    "\n",
    "# target to single label\n",
    "train[\"preprocessed_target\"] = train[\"target\"].apply(lambda x: [1 if str(i) in x.split(\",\") else 0 for i in range(9)])\n",
    "# train[\"preprocessed_target\"] = train[\"target\"].str.split(\",\").apply(lambda x: x[0]).astype(int)\n",
    "# train[\"preprocessed_target\"] = train[\"preprocessed_target\"].apply(lambda x: 0 if x == 0 else (1 if x == 8 else 2))\n",
    "\n",
    "# reset index\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46b07b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (45788, 38), Val Shape: (5088, 38)\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "train, val = train_test_split(train, test_size=0.1, shuffle=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Train Shape: {train.shape}, Val Shape: {val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03545fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, train_mode):\n",
    "        # utils\n",
    "        metadata_columns = [\"salary_rating\", \"team_rating\", \"managment_rating\",\n",
    "                            \"career_rating\", \"workplace_rating\", \"rest_recovery_rating\"]\n",
    "        # for one hot\n",
    "        metadata_columns = [i for i in df.columns if len([j for j in metadata_columns if j in i]) != 0]\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "        # init features\n",
    "        self.positive = df[\"positive\"].values\n",
    "        self.negative = df[\"negative\"].values\n",
    "        self.cities = df[\"city\"].values\n",
    "        self.position = df[\"position\"].values\n",
    "        self.metadata = df[metadata_columns].values\n",
    "        if self.train_mode:\n",
    "            self.target = df[\"preprocessed_target\"].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positive)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get sent\n",
    "        positive, negative = self.positive[idx], self.negative[idx]\n",
    "        city, position = self.cities[idx], self.position[idx]\n",
    "        metadata = self.metadata[idx]\n",
    "        \"\"\"\n",
    "        For text:\n",
    "        \"\"\"      \n",
    "        tokens_positive = TOKENIZER(positive, padding=\"max_length\", truncation=True, max_length=SENT_SIZE, return_tensors=\"pt\")\n",
    "        tokens_negative = TOKENIZER(negative, padding=\"max_length\", truncation=True, max_length=SENT_SIZE, return_tensors=\"pt\")\n",
    "        tokens_city = TOKENIZER(city, padding=\"max_length\", truncation=True, max_length=META_SIZE, return_tensors=\"pt\")\n",
    "        tokens_position = TOKENIZER(position, padding=\"max_length\", truncation=True, max_length=META_SIZE, return_tensors=\"pt\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        For target\n",
    "        \"\"\"        \n",
    "        if self.train_mode:\n",
    "            target = self.target[idx]\n",
    "            return tokens_positive, tokens_negative, tokens_city, tokens_position, torch.FloatTensor(metadata), torch.FloatTensor(target)\n",
    "        else:\n",
    "            return tokens_positive, tokens_negative, tokens_city, tokens_position, torch.FloatTensor(metadata),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b88740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "dataset_train = CustomDataset(train, train_mode=True)\n",
    "dataset_val = CustomDataset(val, train_mode=True)\n",
    "dataset_test = CustomDataset(test, train_mode=False)\n",
    "dataset_fulltrain = CustomDataset(pd.concat((train, val)), train_mode=True)\n",
    "\n",
    "# create dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_fulltrain = DataLoader(dataset_fulltrain, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d10e1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens_positive, tokens_negative, tokens_city, tokens_position, metadata, target in dataloader_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fa722",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f4b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 1e-3\n",
    "FIRST_LEVEL_CLASSES = 3\n",
    "SECOND_LEVEL_CLASSES = 7\n",
    "FULL_CLASSES = 9\n",
    "MODEL_PATH = \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "# MODEL_PATH = \"SkolkovoInstitute/russian_toxicity_classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a50966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_RUBert(is_train: bool = False, model_path: str = \"DeepPavlov/rubert-base-cased\"):\n",
    "    \"\"\"\n",
    "    rubert-base-cased\n",
    "    rubert-base-cased-sentence\n",
    "    distilrubert-base-cased-conversational\n",
    "    \"\"\"\n",
    "    RUBert = BertModel.from_pretrained(model_path)\n",
    "    for i in RUBert.named_parameters():\n",
    "        i[1].requires_grad = is_train\n",
    "    \n",
    "    return RUBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd8a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUBert_positive = BertModel.from_pretrained(MODEL_PATH)\n",
    "# a = RUBert_positive(input_ids=tokens_positive[\"input_ids\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c114c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuBertModel(pl.LightningModule,):\n",
    "    def __init__(self, is_train):\n",
    "        super().__init__()\n",
    "        # utils\n",
    "        self.metric_accuracy = torchmetrics.Accuracy()\n",
    "        self.metric_f1 = torchmetrics.F1(num_classes=FULL_CLASSES, average=\"samples\")\n",
    "        \n",
    "        # logs\n",
    "        self.train_accuracy_log, self.train_f1_log, self.train_loss_log = [], [], []\n",
    "        self.val_accuracy_log, self.val_f1_log, self.val_loss_log = [], [], []\n",
    "        \n",
    "        # RuBert\n",
    "        self.RUBert_positive = init_RUBert(is_train, MODEL_PATH)\n",
    "        self.RUBert_negative = init_RUBert(is_train, MODEL_PATH)\n",
    "        \n",
    "        # Linears\n",
    "        self.linear_metadata = nn.Linear(METADATA_SIZE, 256)\n",
    "        self.linear1_positive = nn.Linear(768, 512)\n",
    "        self.linear1_negative = nn.Linear(768, 512)\n",
    "        self.linear2_positive = nn.Linear(512, 256)\n",
    "        self.linear2_negative = nn.Linear(512, 256)\n",
    "        self.linear_out = nn.Linear(3*256, FULL_CLASSES) \n",
    "        \n",
    "        # utils\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    def forward(self, tokens_positive, tokens_negative, tokens_city, tokens_position, metadata):   \n",
    "        \n",
    "        # RuBert layer\n",
    "        positive_out = self.RUBert_positive(input_ids=tokens_positive[\"input_ids\"].squeeze(1),\n",
    "                                            attention_mask=tokens_positive[\"attention_mask\"].squeeze(1))\n",
    "        positive_out = positive_out[1]\n",
    "        positive_out = torch.reshape(positive_out, (positive_out.shape[0], -1))\n",
    "        positive_out = self.dropout1(positive_out)\n",
    "        negative_out = self.RUBert_negative(input_ids=tokens_negative[\"input_ids\"].squeeze(1),\n",
    "                                            attention_mask=tokens_negative[\"attention_mask\"].squeeze(1))\n",
    "        negative_out = negative_out[1]\n",
    "        negative_out = torch.reshape(negative_out, (negative_out.shape[0], -1))\n",
    "        negative_out = self.dropout1(negative_out)\n",
    "        \n",
    "        # Linear layers\n",
    "        positive_linear = self.relu(self.linear1_positive(positive_out))\n",
    "        negative_linear = self.relu(self.linear1_negative(negative_out))\n",
    "        positive_linear = self.dropout2(positive_linear)\n",
    "        negative_linear = self.dropout2(negative_linear)\n",
    "        positive_linear = self.relu(self.linear2_positive(positive_linear))\n",
    "        negative_linear = self.relu(self.linear2_negative(negative_linear))\n",
    "        metadata = self.linear_metadata(metadata)\n",
    "        \n",
    "        x = torch.cat((positive_linear, negative_linear, metadata), dim=-1)\n",
    "        \n",
    "        # Output\n",
    "        out = self.linear_out(x)\n",
    "        out = torch.nn.Sigmoid()(out)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        #learning rate scheduler\n",
    "        return {\"optimizer\":optimizer,\n",
    "                \"lr_scheduler\" : {\"scheduler\" : sch}\n",
    "               }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tokens_positive, tokens_negative, tokens_city, tokens_position, metadata, target = batch\n",
    "        out = self(tokens_positive, tokens_negative, tokens_city, tokens_position, metadata,)\n",
    "        loss = torch.nn.BCELoss()(out, target)\n",
    "        accuracy = self.metric_accuracy(out, target.int())\n",
    "        f1 = self.metric_f1(out, target.int())\n",
    "        \n",
    "        # save logs\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", accuracy, prog_bar=True)\n",
    "        self.log(\"train_f1\", f1, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss, \"accuracy\": accuracy, \"F1\":f1}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tokens_positive, tokens_negative, tokens_city, tokens_position, metadata, target = batch\n",
    "        out = self(tokens_positive, tokens_negative, tokens_city, tokens_position, metadata,)        \n",
    "        loss = torch.nn.BCELoss()(out, target)\n",
    "        accuracy = self.metric_accuracy(out, target.int())\n",
    "        f1 = self.metric_f1(out, target.int())\n",
    "        \n",
    "        # save logs\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_accuracy\", accuracy, prog_bar=True)\n",
    "        self.log(\"val_f1\", f1, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss, \"accuracy\": accuracy, \"F1\":f1}\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        tokens_positive, tokens_negative, tokens_city, tokens_position, metadata = batch\n",
    "        out = self(tokens_positive, tokens_negative, tokens_city, tokens_position, metadata)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def training_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.train_loss_log.append(np.mean([i[\"loss\"].item() for i in outs]))\n",
    "        self.train_accuracy_log.append(np.mean([i[\"accuracy\"].cpu() for i in outs]))\n",
    "        self.train_f1_log.append(np.mean([i[\"F1\"].cpu() for i in outs]))\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.val_loss_log.append(np.mean([i[\"loss\"].item() for i in outs]))\n",
    "        self.val_accuracy_log.append(np.mean([i[\"accuracy\"].cpu() for i in outs]))\n",
    "        self.val_f1_log.append(np.mean([i[\"F1\"].cpu() for i in outs]))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a6e02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bert_model = RuBertModel(is_train=False)\n",
    "# checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode = \"min\", dirpath=\"data/\", filename=\"RuBert\")\n",
    "# logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\", name=\"RuBert\", version=1)\n",
    "\n",
    "# trainer = pl.Trainer(gpus=1, max_epochs=11, logger=logger,\n",
    "#                      default_root_dir=\"data/\")\n",
    "# trainer.fit(bert_model, dataloader_train, dataloader_val)\n",
    "\n",
    "# # save model\n",
    "# trainer.save_checkpoint(\"data/models/RUBert.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82210cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # save test preds\n",
    "# preds = trainer.predict(bert_model, dataloader_test)\n",
    "# submit = []\n",
    "# thresh = 0.3\n",
    "\n",
    "# for pred in tqdm(preds):\n",
    "#     pred_batch = torch.where((pred > thresh)[0])[0].detach().tolist()\n",
    "#     pred_batch = \",\".join([str(i) for i in pred_batch])\n",
    "#     if pred_batch == '':\n",
    "#         print(pred)\n",
    "#     submit.append(pred_batch)\n",
    "    \n",
    "# sample_submission[\"target\"] = submit\n",
    "# sample_submission.to_csv(\"data/submissions/submission_rubert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7051d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bert_model = RuBertModel(is_train=True)\n",
    "# # checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode = \"min\", dirpath=\"data/\", filename=\"RuBert\")\n",
    "# logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\", name=\"RuBert_retrained\", version=1)\n",
    "\n",
    "# trainer = pl.Trainer(gpus=1, max_epochs=7, logger=logger,\n",
    "#                      default_root_dir=\"data/\")\n",
    "# trainer.fit(bert_model, dataloader_train, dataloader_val)\n",
    "\n",
    "# # save model\n",
    "# trainer.save_checkpoint(\"data/models/RUBert_retrained.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "979a966d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.05 s, sys: 2.35 s, total: 8.4 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# bert_model = RuBertModel.load_from_checkpoint(\"data/models/RUBert_retrained.ckpt\", is_train=True)\n",
    "# # checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode = \"min\", dirpath=\"data/\", filename=\"RuBert\")\n",
    "# logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\", name=\"RuBert_retrained\", version=1)\n",
    "\n",
    "# trainer = pl.Trainer(gpus=1, max_epochs=7, logger=logger,\n",
    "#                      default_root_dir=\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe45a428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2b1c5e712f49568289c7cd42034acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 11s, sys: 825 ms, total: 10min 12s\n",
      "Wall time: 10min 11s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# preds = trainer.predict(bert_model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d15c2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # save test preds\n",
    "# submit = []\n",
    "# thresh = 0.3\n",
    "\n",
    "# for pred in tqdm(preds):\n",
    "#     pred_batch = torch.where((pred > thresh)[0])[0].detach().tolist()\n",
    "#     pred_batch = \",\".join([str(i) for i in pred_batch])\n",
    "#     if pred_batch == '':\n",
    "#         print(pred)\n",
    "#     submit.append(pred_batch)\n",
    "    \n",
    "# sample_submission[\"target\"] = submit\n",
    "# sample_submission.to_csv(\"data/submissions/submission_rubert_retrained.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1a195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a27ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b4874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
