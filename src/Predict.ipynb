{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import fasttext\n",
    "from navec import Navec\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.functional as F\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b2bbd",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13490cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CustomDataset, FinalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "navec_model = Navec.load(\"data/navec_hudlit_v1_12B_500K_300d_100q.tar\")\n",
    "fasttext_model = fasttext.load_model(\"data/cc.ru.300.bin\")\n",
    "\n",
    "# # load test and sort\n",
    "# test_data = glob(\"data/augmentations/test/*.npy\")\n",
    "# test_data = [i.split(\"_\") for i in test_data]\n",
    "# test_data = {i[1]: i[0] for i in test_data}\n",
    "# sorted_test_data = []\n",
    "# for i in range(len(test_data)):\n",
    "#     sorted_test_data.append(test_data[str(i)+\".npy\"]+\"_\"+str(i)+\".npy\")\n",
    "    \n",
    "# load new_test and sort\n",
    "new_test_data = glob(\"data/augmentations/new_test/*.npy\")\n",
    "new_test_data = [[\"/\".join(i.split(\"/\")[:-1]), i.split(\"_\")[-1]] for i in new_test_data]\n",
    "new_test_data = {i[1]: i[0] for i in new_test_data}\n",
    "sorted_test_data = []\n",
    "for i in range(len(new_test_data)):\n",
    "    sorted_test_data.append(new_test_data[str(i)+\".npy\"] + \"/\" + \"test_\" + str(i) + \".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11735926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "sent_size = 112\n",
    "batch_size = 128\n",
    "\n",
    "dataset_test = CustomDataset(sorted_test_data, sent_size, False, navec_model, fasttext_model)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618a71b",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ae662",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = []\n",
    "\n",
    "for idx in range(4):\n",
    "\n",
    "    model = FinalModel.load_from_checkpoint(f\"data/models/final_model_{idx}.ckpt\")\n",
    "    trainer = pl.Trainer(gpus=1)\n",
    "    \n",
    "    preds = trainer.predict(model, dataloader_test)\n",
    "    preds_list.append(preds)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1 = np.asarray([j.numpy().tolist() for i in preds_list[0] for j in i])\n",
    "preds2 = np.asarray([j.numpy().tolist() for i in preds_list[1] for j in i])\n",
    "preds3 = np.asarray([j.numpy().tolist() for i in preds_list[2] for j in i])\n",
    "preds4 = np.asarray([j.numpy().tolist() for i in preds_list[3] for j in i])\n",
    "preds5 = np.asarray([j.numpy().tolist() for i in preds_list[3] for j in i])\n",
    "\n",
    "preds = (preds1 + preds2 + preds3 + preds4 + preds5) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts\n",
    "sample_submission = pd.read_csv(\"data/HeadHunter_sample_submit.csv\")\n",
    "\n",
    "const = 0.4\n",
    "thresholds = [const]\n",
    "y_pred = []\n",
    "submit_preds = []\n",
    "count_zero = 0\n",
    "\n",
    "for pred in tqdm(preds):\n",
    "    pred = (pred > thresholds).astype(int).tolist()\n",
    "    y_pred.extend(pred)\n",
    "    \n",
    "    if sum(pred) == 0:\n",
    "        count_zero += 1\n",
    "        submit_preds.append(\"0\")\n",
    "    else:\n",
    "        submit_preds.append(\",\".join([str(i) for i in range(9) if pred[i]==1]))\n",
    "        \n",
    "print(f\"Zero forecasts: {count_zero}\")\n",
    "sample_submission[\"target\"] = submit_preds\n",
    "sample_submission.to_csv(\"data/submissions/final_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4131e23",
   "metadata": {},
   "source": [
    "## Predict new test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd22ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalModel.load_from_checkpoint(f\"data/models/final_model_3.ckpt\")\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "\n",
    "preds = trainer.predict(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0dffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.asarray([j.numpy().tolist() for i in preds for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts\n",
    "sample_submission = pd.read_csv(\"data/HeadHunter_new_train.csv\")\n",
    "\n",
    "const = 0.5\n",
    "thresholds = [const]\n",
    "y_pred = []\n",
    "submit_preds = []\n",
    "count_zero = 0\n",
    "\n",
    "for pred in tqdm(preds):\n",
    "    pred = (pred > thresholds).astype(int).tolist()\n",
    "    y_pred.extend(pred)\n",
    "    \n",
    "    if sum(pred) == 0:\n",
    "        count_zero += 1\n",
    "        submit_preds.append(\"0\")\n",
    "    else:\n",
    "        temp = \",\".join([str(i) for i in range(9) if pred[i]==1])\n",
    "        submit_preds.append(temp)\n",
    "        \n",
    "print(f\"Zero forecasts: {count_zero}\")\n",
    "sample_submission[\"preds\"] = submit_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sample_submission[\"target\"] = sample_submission[\"target\"].apply(\n",
    "        lambda x: [1 if str(i) in x.split(\",\") else 0 for i in range(9)]\n",
    "    )\n",
    "sample_submission[\"preds\"] = sample_submission[\"preds\"].apply(\n",
    "        lambda x: [1 if str(i) in x.split(\",\") else 0 for i in range(9)]\n",
    "    )\n",
    "\n",
    "y_true = sample_submission[\"target\"].values\n",
    "y_pred = sample_submission[\"preds\"].values\n",
    "\n",
    "f1_score(np.array([i for i in y_true]), np.array([i for i in y_pred]), average=\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18aa112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without '0,8'\n",
    "y_true_res = []\n",
    "y_pred_res = []\n",
    "\n",
    "for idx, data in sample_submission.iterrows():\n",
    "    if data[\"preds\"][0] == 1 and data[\"preds\"][-1] == 1:\n",
    "        continue\n",
    "    \n",
    "    y_true_res.append(data[\"target\"])\n",
    "    y_pred_res.append(data[\"preds\"])\n",
    "    \n",
    "f1_score(np.array([i for i in y_true_res]), np.array([i for i in y_pred_res]), average=\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193fe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7e736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf3964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
