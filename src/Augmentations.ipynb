{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d8f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel\n",
    "import nltk\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27423f2c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084defc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/HeadHunter_train.csv\")\n",
    "test = pd.read_csv(\"data/HeadHunter_test.csv\")\n",
    "new_test = pd.read_csv(\"data/HeadHunter_new_train.csv\")\n",
    "sample_submission = pd.read_csv(\"data/HeadHunter_sample_submit.csv\")\n",
    "\n",
    "print(f\"Train shape: {train.shape} | Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6d577",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9476fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import preprocessing, get_vocab, augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d281d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train, test = preprocessing(train.copy(), test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "train, val = train_test_split(train, test_size=0.1)\n",
    "\n",
    "print(f\"Train Size: {train.shape}, Val Size: {val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Augmentations\n",
    "train_positive_sentences, train_negative_sentences, train_meta, train_labels = augmentations(train)\n",
    "\n",
    "np.save(\"data/augmentations/train_positive_sentences\", np.asarray(train_positive_sentences))\n",
    "np.save(\"data/augmentations/train_negative_sentences\", np.asarray(train_negative_sentences))\n",
    "np.save(\"data/augmentations/train_meta\", np.asarray(train_meta))\n",
    "np.save(\"data/augmentations/train_labels\", np.asarray(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287043ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val Augmentations\n",
    "val_positive_sentences, val_negative_sentences, val_meta, val_labels = augmentations(val, is_test=True)\n",
    "\n",
    "np.save(\"data/augmentations/val_positive_sentences\", np.asarray(val_positive_sentences))\n",
    "np.save(\"data/augmentations/val_negative_sentences\", np.asarray(val_negative_sentences))\n",
    "np.save(\"data/augmentations/val_meta\", np.asarray(val_meta))\n",
    "np.save(\"data/augmentations/val_labels\", np.asarray(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test augmentations\n",
    "test_positive_sentences, test_negative_sentences, test_meta, _ = augmentations(test, is_test=True)\n",
    "\n",
    "np.save(\"data/augmentations/test_positive_sentences\", np.asarray(test_positive_sentences))\n",
    "np.save(\"data/augmentations/test_negative_sentences\", np.asarray(test_negative_sentences))\n",
    "np.save(\"data/augmentations/test_meta\", np.asarray(test_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c663fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Augmentations (as test)\n",
    "train_positive_sentences, train_negative_sentences, train_meta, train_labels = augmentations(train, is_test=True)\n",
    "\n",
    "np.save(\"data/augmentations/train2_positive_sentences\", np.asarray(train_positive_sentences))\n",
    "np.save(\"data/augmentations/train2_negative_sentences\", np.asarray(train_negative_sentences))\n",
    "np.save(\"data/augmentations/train2_meta\", np.asarray(train_meta))\n",
    "np.save(\"data/augmentations/train2_labels\", np.asarray(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a17028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New test\n",
    "train, test = preprocessing(train.copy(), new_test.copy())\n",
    "\n",
    "# Test augmentations\n",
    "test_positive_sentences, test_negative_sentences, test_meta, _ = augmentations(test, is_test=True)\n",
    "\n",
    "np.save(\"data/augmentations/new_test_positive_sentences\", np.asarray(test_positive_sentences))\n",
    "np.save(\"data/augmentations/new_test_negative_sentences\", np.asarray(test_negative_sentences))\n",
    "np.save(\"data/augmentations/new_test_meta\", np.asarray(test_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4269f",
   "metadata": {},
   "source": [
    "## Save to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e50ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train\n",
    "train_positive_sentences = np.load(\"data/augmentations/train_positive_sentences.npy\")\n",
    "train_negative_sentences = np.load(\"data/augmentations/train_negative_sentences.npy\")\n",
    "train_meta = np.load(\"data/augmentations/train_meta.npy\")\n",
    "train_labels = np.load(\"data/augmentations/train_labels.npy\")\n",
    "\n",
    "# load val\n",
    "val_positive_sentences = np.load(\"data/augmentations/val_positive_sentences.npy\")\n",
    "val_negative_sentences = np.load(\"data/augmentations/val_negative_sentences.npy\")\n",
    "val_meta = np.load(\"data/augmentations/val_meta.npy\")\n",
    "val_labels = np.load(\"data/augmentations/val_labels.npy\")\n",
    "\n",
    "# load test\n",
    "test_positive_sentences = np.load(\"data/augmentations/test_positive_sentences.npy\")\n",
    "test_negative_sentences = np.load(\"data/augmentations/test_negative_sentences.npy\")\n",
    "test_meta = np.load(\"data/augmentations/test_meta.npy\")\n",
    "\n",
    "# load new test\n",
    "new_test_positive_sentences = np.load(\"data/augmentations/new_test_positive_sentences.npy\")\n",
    "new_test_negative_sentences = np.load(\"data/augmentations/new_test_negative_sentences.npy\")\n",
    "new_test_meta = np.load(\"data/augmentations/new_test_meta.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8703a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train\n",
    "for i in tqdm(range(len(train_positive_sentences))):\n",
    "    batch = np.array([train_positive_sentences[i], train_negative_sentences[i], list(train_meta[i]),\n",
    "                  list(train_labels[i])], dtype=\"object\")\n",
    "    np.save(f\"data/augmentations/train/train_{i}\", batch)\n",
    "    \n",
    "# save val\n",
    "for i in tqdm(range(len(val_positive_sentences))):\n",
    "    batch = np.array([val_positive_sentences[i], val_negative_sentences[i], list(val_meta[i]),\n",
    "                  list(val_labels[i])], dtype=\"object\")\n",
    "    np.save(f\"data/augmentations/val/val_{i}\", batch)\n",
    "    \n",
    "# save test\n",
    "for i in tqdm(range(len(test_positive_sentences))):\n",
    "    batch = np.array([test_positive_sentences[i], test_negative_sentences[i], list(test_meta[i])], dtype=\"object\")\n",
    "    np.save(f\"data/augmentations/test/test_{i}\", batch)\n",
    "    \n",
    "# save new test\n",
    "for i in tqdm(range(len(new_test_positive_sentences))):\n",
    "    batch = np.array([new_test_positive_sentences[i], new_test_negative_sentences[i], list(new_test_meta[i])], dtype=\"object\")\n",
    "    np.save(f\"data/augmentations/new_test/test_{i}\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train\n",
    "train_positive_sentences = np.load(\"data/augmentations/train2_positive_sentences.npy\")\n",
    "train_negative_sentences = np.load(\"data/augmentations/train2_negative_sentences.npy\")\n",
    "train_meta = np.load(\"data/augmentations/train2_meta.npy\")\n",
    "train_labels = np.load(\"data/augmentations/train2_labels.npy\")\n",
    "\n",
    "# save train\n",
    "for i in tqdm(range(len(train_positive_sentences))):\n",
    "    batch = np.array([train_positive_sentences[i], train_negative_sentences[i], list(train_meta[i]),\n",
    "                  list(train_labels[i])], dtype=\"object\")\n",
    "    np.save(f\"data/augmentations/train2/train_{i}\", batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef9ac3",
   "metadata": {},
   "source": [
    "## Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b58e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LSTMModel, Model, CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb321964",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_size = 112\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "tokenizer_lstm = nltk.RegexpTokenizer(r\"[а-я]+|<unk>|<pad>\")\n",
    "\n",
    "# load test and sort\n",
    "test_data = glob(\"data/augmentations/test/*.npy\")\n",
    "\n",
    "# data\n",
    "dataset_test = CustomDataset(test_data, tokenizer_bert, tokenizer_lstm, sent_size=sent_size,\n",
    "                              train_mode=False, model_type=\"lstm\")\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = LSTMModel.load_from_checkpoint(\"data/models/Final_Model_lstm.ckpt\")\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "# preds\n",
    "preds = trainer.predict(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ef00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preds2Classes\n",
    "const = 0.2\n",
    "thresholds = [const]\n",
    "y_pred = []\n",
    "count_zero = 0\n",
    "\n",
    "for pred in preds:\n",
    "    pred = (pred.numpy() > thresholds).astype(int).tolist()\n",
    "    y_pred.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pseudo labels\n",
    "for idx, data_path in enumerate(test_data):\n",
    "    pseudo_test = np.load(data_path, allow_pickle=True).tolist()\n",
    "    pseudo_test.append(y_pred[idx])\n",
    "    pseudo_test = np.array(pseudo_test, dtype=\"object\")    \n",
    "    np.save(f\"data/augmentations/test_pseudo/test_{idx}\", pseudo_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5523f",
   "metadata": {},
   "source": [
    "## Change target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train\n",
    "train_positive_sentences = np.load(\"data/augmentations/train_positive_sentences.npy\")\n",
    "train_negative_sentences = np.load(\"data/augmentations/train_negative_sentences.npy\")\n",
    "train_meta = np.load(\"data/augmentations/train_meta.npy\")\n",
    "train_labels = np.load(\"data/augmentations/train_labels.npy\")\n",
    "\n",
    "# load val\n",
    "val_positive_sentences = np.load(\"data/augmentations/val_positive_sentences.npy\")\n",
    "val_negative_sentences = np.load(\"data/augmentations/val_negative_sentences.npy\")\n",
    "val_meta = np.load(\"data/augmentations/val_meta.npy\")\n",
    "val_labels = np.load(\"data/augmentations/val_labels.npy\")\n",
    "\n",
    "# load test\n",
    "test_positive_sentences = np.load(\"data/augmentations/test_positive_sentences.npy\")\n",
    "test_negative_sentences = np.load(\"data/augmentations/test_negative_sentences.npy\")\n",
    "test_meta = np.load(\"data/augmentations/test_meta.npy\")\n",
    "\n",
    "# load test\n",
    "new_test_positive_sentences = np.load(\"data/augmentations/new_test_positive_sentences.npy\")\n",
    "new_test_negative_sentences = np.load(\"data/augmentations/new_test_negative_sentences.npy\")\n",
    "new_test_meta = np.load(\"data/augmentations/new_test_meta.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba72940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train\n",
    "for i in tqdm(range(len(train_positive_sentences))):\n",
    "    batch = np.array([train_positive_sentences[i], train_negative_sentences[i], list(train_meta[i]),\n",
    "                  list(train_labels[i])], dtype=\"object\")\n",
    "    batch[-1] = batch[-1][1:]\n",
    "    np.save(f\"data/augmentations/train/train_{i}\", batch)\n",
    "    \n",
    "# # save val\n",
    "for i in tqdm(range(len(val_positive_sentences))):\n",
    "    batch = np.array([val_positive_sentences[i], val_negative_sentences[i], list(val_meta[i]),\n",
    "                  list(val_labels[i])], dtype=\"object\")\n",
    "    batch[-1] = batch[-1][1:]\n",
    "    np.save(f\"data/augmentations/val/val_{i}\", batch)\n",
    "    \n",
    "# save test\n",
    "for i in tqdm(range(len(test_positive_sentences))):\n",
    "    batch = np.array([test_positive_sentences[i], test_negative_sentences[i], list(test_meta[i])], dtype=\"object\")\n",
    "    np.save(f\"data/augmentations/test/test_{i}\", batch)\n",
    "    \n",
    "    \n",
    "# save new_test\n",
    "for i in tqdm(range(len(new_test_positive_sentences))):\n",
    "    batch = np.array([new_test_positive_sentences[i], new_test_negative_sentences[i], list(new_test_meta[i])], dtype=\"object\")\n",
    "    np.save(f\"data/augmentations/new_test/test_{i}\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9188a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f18f678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bda5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
